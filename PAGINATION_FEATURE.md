# å¤šåˆ†é¡µå†…å®¹æå–åŠŸèƒ½

## æ¦‚è¿°

å·²æˆåŠŸå®ç° t66y è®ºå›çš„**å¤šåˆ†é¡µå†…å®¹æå–**åŠŸèƒ½ã€‚å½“ä¸€ç¯‡å¸–å­å­˜åœ¨å¤šä¸ªåˆ†é¡µï¼ˆå¦‚é¡µé¢ 1-6ï¼‰æ—¶ï¼Œçˆ¬è™«ç°åœ¨èƒ½å¤Ÿè‡ªåŠ¨ä»æ‰€æœ‰åˆ†é¡µä¸­æå–å®Œæ•´å†…å®¹ã€‚

## é—®é¢˜èƒŒæ™¯

åŸå§‹çˆ¬è™«å­˜åœ¨çš„é—®é¢˜ï¼š
- ğŸ”´ ä»…æå–ç¬¬ä¸€é¡µçš„å†…å®¹
- ğŸ”´ å½“ä¸€ç¯‡å¸–å­è·¨è¶Šå¤šä¸ªåˆ†é¡µæ—¶ï¼Œç¬¬2-Né¡µçš„å†…å®¹è¢«é—æ¼
- ğŸ”´ å¯¼è‡´å†…å®¹ä¸å®Œæ•´

## è§£å†³æ–¹æ¡ˆ

### 1. **åˆ†é¡µæ£€æµ‹** (`extract_page_numbers`)
ä»é¦–é¡µ HTML ä¸­è‡ªåŠ¨æ£€æµ‹æ€»åˆ†é¡µæ•°ï¼š
```python
def extract_page_numbers(self, html):
    """ä»HTMLä¸­æå–æ€»é¡µæ•°"""
    soup = BeautifulSoup(html, 'html.parser')
    all_links = soup.find_all('a')
    page_numbers = set()
    
    # æŸ¥æ‰¾æ‰€æœ‰ page=N çš„é“¾æ¥
    for link in all_links:
        href = link.get('href', '')
        match = re.search(r'page=(\d+)', href)
        if match:
            page_numbers.add(int(match.group(1)))
    
    return max(page_numbers) if page_numbers else 1
```

**åŸç†**ï¼št66y è®ºå›çš„åˆ†é¡µå¯¼èˆªä¸­åŒ…å«å½¢å¦‚ `read.php?tid=XXX&page=N` çš„é“¾æ¥ã€‚é€šè¿‡æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰é¡µç ï¼Œå¾—åˆ°æœ€å¤§é¡µç å³ä¸ºæ€»é¡µæ•°ã€‚

### 2. **URL æ„å»º** (`build_pagination_url`)
ä¸ºä»»ä½•ç»™å®šçš„é¡µç æ„å»ºæœ‰æ•ˆçš„è®¿é—® URLï¼š
```python
def build_pagination_url(self, original_url, page_num):
    """ä¸ºç»™å®šé¡µç æ„å»ºURL"""
    tid = self.extract_tid_from_url(original_url)
    if tid:
        return f"https://t66y.com/read.php?tid={tid}&page={page_num}"
    return None
```

**åŸç†**ï¼šä»åŸå§‹ URL ä¸­æå– thread IDï¼Œç„¶åæ„å»ºæ ‡å‡†çš„åˆ†é¡µé“¾æ¥ã€‚æ”¯æŒä¸¤ç§URLæ ¼å¼ï¼š
- ç›´æ¥åˆ†é¡µ URLï¼š`read.php?tid=7027882&page=1` â†’ ç›´æ¥æå– tid
- HTML æ•°æ® URLï¼š`htm_data/2511/20/7027882.html` â†’ ä»è·¯å¾„æå– tid

### 3. **å¤šé¡µéå†** (`parse_t66y_post`)
æ ¸å¿ƒé€»è¾‘ï¼š
```python
def parse_t66y_post(self, url, html, task_type='image'):
    # ç¬¬1æ­¥ï¼šæå–ç¬¬ä¸€é¡µå†…å®¹
    all_content_parts, all_images = self._extract_page_content(html, [], [], page_num=1)
    
    # ç¬¬2æ­¥ï¼šæ£€æµ‹æ€»é¡µæ•°
    total_pages = self.extract_page_numbers(html)  # ä¾‹å¦‚ï¼š6
    
    # ç¬¬3æ­¥ï¼šéå†å…¶ä»–é¡µé¢
    if total_pages > 1:
        for page_num in range(2, total_pages + 1):
            page_url = self.build_pagination_url(url, page_num)
            page_html = self.fetch_page(page_url)
            all_content_parts, all_images = self._extract_page_content(
                page_html, all_content_parts, all_images, page_num=page_num
            )
    
    # ç¬¬4æ­¥ï¼šåˆå¹¶æ‰€æœ‰å†…å®¹
    content = '\n\n'.join(all_content_parts)
    return {..., 'content': content, ...}
```

**æµç¨‹**ï¼š
1. è·å–å¹¶è§£æç¬¬ä¸€é¡µ HTML
2. ä»åˆ†é¡µå¯¼èˆªä¸­æ£€æµ‹æ€»é¡µæ•°
3. å¦‚æœ `æ€»é¡µæ•° > 1`ï¼Œåˆ™é€é¡µè·å–å‰©ä½™é¡µé¢
4. æ¯é¡µå†…å®¹ç‹¬ç«‹æå–ï¼Œä½¿ç”¨ `\n\n`ï¼ˆåŒæ¢è¡Œï¼‰åˆ†éš”
5. è¿”å›åˆå¹¶åçš„å®Œæ•´å†…å®¹

### 4. **å•é¡µå†…å®¹æå–** (`_extract_page_content`)
ä»å•ä¸ªé¡µé¢æå–æ‰€æœ‰æ¥¼å±‚ï¼š
```python
def _extract_page_content(self, html, content_parts, images, page_num=1):
    """ä»å•ä¸ªé¡µé¢HTMLä¸­æå–å†…å®¹å’Œå›¾ç‰‡"""
    soup = BeautifulSoup(html, 'html.parser')
    content_divs = soup.find_all('div', class_='tpc_content')
    
    for floor_idx, content_div in enumerate(content_divs, 1):
        text_content = content_div.get_text(strip=True)
        if text_content:
            content_parts.append(text_content)
        
        # åŒæ—¶æå–è¯¥æ¥¼å±‚çš„å›¾ç‰‡
        img_elements = content_div.find_all('img')
        for img in img_elements:
            img_url = img.get('ess-data') or img.get('src')
            if img_url and img_url.startswith('http'):
                if img_url not in [i['url'] for i in images]:
                    images.append({
                        'url': img_url,
                        'description': f'ç¬¬{page_num}é¡µ æ¥¼å±‚{floor_idx}'
                    })
    
    return content_parts, images
```

## æµ‹è¯•ç»“æœ

### æµ‹è¯•ç”¨ä¾‹ï¼š`https://t66y.com/htm_data/2511/20/7027882.html`

**å¸–å­ä¿¡æ¯**ï¼š
- æ ‡é¢˜ï¼š`[ç¾ä»£å¥‡å¹»] æ€§æ„Ÿå°¤ç‰©è€å¸ˆå¦ˆå¦ˆç‹è¶Š1-12`
- æ€»åˆ†é¡µæ•°ï¼š**6 é¡µ**
- æ€»æ¥¼å±‚æ•°ï¼š**6 ä¸ªæ¥¼å±‚**ï¼ˆåˆ†å¸ƒåœ¨å¤šä¸ªé¡µé¢ï¼‰

**æå–ç»“æœ**ï¼š
| æ¥¼å±‚ | å­—ç¬¦æ•° | æ‰€åœ¨é¡µé¢ |
|------|--------|---------|
| æ¥¼å±‚1 | 13,961 | é¡µé¢2 |
| æ¥¼å±‚2 | 18,326 | é¡µé¢2 |
| æ¥¼å±‚3 | 17,704 | é¡µé¢3 |
| æ¥¼å±‚4 | 50 | é¡µé¢4 |
| æ¥¼å±‚5 | 33 | é¡µé¢5 |
| æ¥¼å±‚6 | 44 | é¡µé¢6 |
| **åˆè®¡** | **49,995** | **6 é¡µ** |

âœ… **éªŒè¯**ï¼šæ‰€æœ‰å†…å®¹å‡å·²æ­£ç¡®æå–å¹¶å­˜å‚¨è‡³ MongoDB

### æ‰§è¡Œæ—¥å¿—ç¤ºä¾‹
```
ğŸ“„ å¼€å§‹æå–ç¬¬ä¸€é¡µå†…å®¹...
ğŸ“Š æ£€æµ‹åˆ°æ€»é¡µæ•°: 6
ğŸ”„ å¤šåˆ†é¡µæ¨¡å¼ï¼šå¼€å§‹éå†ç¬¬ 2-6 é¡µ...
  â†’ ç¬¬ 2 é¡µ: æå–ä¸­...
  â†’ ç¬¬ 3 é¡µ: æå–ä¸­...
  â†’ ç¬¬ 4 é¡µ: æå–ä¸­...
  â†’ ç¬¬ 5 é¡µ: æå–ä¸­...
  â†’ ç¬¬ 6 é¡µ: æå–ä¸­...
âœ“ è·å–æ¥¼ä¸»æ–‡æœ¬å†…å®¹: 49995 å­—ç¬¦
âœ“ æ–‡ç« å·²ä¿å­˜
```

## æ ¸å¿ƒæ”¹åŠ¨

**æ–‡ä»¶**ï¼š`/workspaces/forum-crawler-service/crawler/crawl.py`

### æ–°å¢æ–¹æ³•ï¼š
1. `extract_page_numbers(html)` - æ£€æµ‹æ€»åˆ†é¡µæ•°
2. `extract_tid_from_url(url)` - ä»URLæå–çº¿ç¨‹ID
3. `build_pagination_url(url, page_num)` - æ„å»ºåˆ†é¡µURL
4. `_extract_page_content(html, content_parts, images, page_num)` - æå–å•é¡µå†…å®¹

### ä¿®æ”¹æ–¹æ³•ï¼š
- `parse_t66y_post()` - å¢åŠ å¤šé¡µéå†é€»è¾‘
- `fetch_page()` - æ”¯æŒå¤šé¡µè¯·æ±‚

## æ€§èƒ½ç‰¹æ€§

âœ… **æ™ºèƒ½æ£€æµ‹**ï¼šæ— éœ€æ‰‹åŠ¨é…ç½®ï¼Œè‡ªåŠ¨æ£€æµ‹åˆ†é¡µæ•°  
âœ… **å®¹é”™å¤„ç†**ï¼šå•é¡µè·å–å¤±è´¥æ—¶ç»§ç»­å¤„ç†ä¸‹ä¸€é¡µ  
âœ… **å»é‡æœºåˆ¶**ï¼šé¿å…é‡å¤æ·»åŠ ç›¸åŒçš„å›¾ç‰‡URL  
âœ… **å‘åå…¼å®¹**ï¼šå•é¡µå¸–å­ç»§ç»­æ­£å¸¸å¤„ç†  
âœ… **æ—¥å¿—è¯¦å°½**ï¼šå®Œæ•´çš„å¤šé¡µå¤„ç†æ—¥å¿—è¾“å‡º  

## é™åˆ¶ä¸çº¦æŸ

- â±ï¸ æ¯é¡µ HTTP è¯·æ±‚è¶…æ—¶ï¼š10 ç§’
- ğŸ”— è‡ªåŠ¨åœæ­¢ï¼šå½“è¿ç»­è·å–é¡µé¢å¤±è´¥æ—¶åœæ­¢
- ğŸ“Š æœ€å¤§åˆ†é¡µï¼šç†è®ºä¸Šæ— é™åˆ¶ï¼ˆå—è¶…æ—¶é™åˆ¶å½±å“ï¼‰
- ğŸ–¼ï¸ å›¾ç‰‡è¿‡æ»¤ï¼šè‡ªåŠ¨è¿‡æ»¤è¡¨æƒ…ã€å¤´åƒã€å›¾æ ‡ç­‰å°å›¾ç‰‡

## éƒ¨ç½²çŠ¶æ€

âœ… **å·²éƒ¨ç½²**ï¼šDocker å®¹å™¨ä¸­çš„çˆ¬è™«å·²æ›´æ–°  
âœ… **å·²æµ‹è¯•**ï¼šé€šè¿‡å¤šä¸ªæµ‹è¯•ç”¨ä¾‹éªŒè¯åŠŸèƒ½  
âœ… **ç”Ÿäº§å°±ç»ª**ï¼šå¯åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨  

## ä½¿ç”¨ç¤ºä¾‹

### åˆ›å»ºå¤šåˆ†é¡µçˆ¬è™«ä»»åŠ¡
```bash
curl -X POST http://localhost:5000/api/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "name": "å¤šåˆ†é¡µæå–æµ‹è¯•",
    "forumUrl": "https://t66y.com/htm_data/2511/20/7027882.html",
    "taskType": "novel"
  }'
```

**é¢„æœŸç»“æœ**ï¼šè‡ªåŠ¨ä»æ‰€æœ‰ 6 ä¸ªåˆ†é¡µä¸­æå–æ‰€æœ‰å†…å®¹ï¼Œæ€»å…± 49,995 å­—ç¬¦ã€‚

## æœªæ¥ä¼˜åŒ–

- [ ] å¹¶è¡Œå¤šé¡µè¯·æ±‚ï¼ˆæé«˜é€Ÿåº¦ï¼‰
- [ ] å¯é…ç½®çš„åˆ†é¡µèŒƒå›´é™åˆ¶
- [ ] åˆ†é¡µè¿›åº¦å®æ—¶æŠ¥å‘Š
- [ ] æ–­ç‚¹ç»­çˆ¬æœºåˆ¶

---

**æœ€åæ›´æ–°**ï¼š2025-12-02  
**ç‰ˆæœ¬**ï¼š2.0 - å¤šåˆ†é¡µæ”¯æŒç‰ˆæœ¬
