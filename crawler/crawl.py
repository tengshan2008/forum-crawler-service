#!/usr/bin/env python3
"""
è®ºå›çˆ¬è™«å¯åŠ¨è„šæœ¬
çœŸå®çˆ¬è™«å®ç°ï¼Œçˆ¬å–è®ºå›æ•°æ®å¹¶ä¿å­˜åˆ° MongoDB
"""

import sys
import os
import argparse
import json
import time
import requests
from datetime import datetime, timezone
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import re
import logging

# å¯¼å…¥ MongoDB å®¢æˆ·ç«¯
from pymongo import MongoClient
from bson import ObjectId

# å¯¼å…¥å›¾ç‰‡ä¸‹è½½å™¨
from image_downloader import download_images, initialize_image_dirs

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

class ForumCrawler:
    """çœŸå®çš„è®ºå›çˆ¬è™«å®ç°"""
    
    def __init__(self, task_id, mongodb_uri):
        self.task_id = task_id
        self.mongodb_uri = mongodb_uri
        self.client = None
        self.db = None
        self.posts_collection = None
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.connect_db()
    
    def connect_db(self):
        """è¿æ¥ MongoDB"""
        try:
            self.client = MongoClient(self.mongodb_uri, serverSelectionTimeoutMS=5000)
            self.client.admin.command('ping')
            self.db = self.client['forum-crawler']
            self.posts_collection = self.db['posts']
            print(f"âœ“ MongoDB è¿æ¥æˆåŠŸ", flush=True)
        except Exception as e:
            print(f"âœ— MongoDB è¿æ¥å¤±è´¥: {e}", file=sys.stderr, flush=True)
            raise
    
    def fetch_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"âœ— è·å–é¡µé¢å¤±è´¥ {url}: {e}", file=sys.stderr, flush=True)
            return None
    
    def extract_page_numbers(self, html):
        """ä»HTMLä¸­æå–æ€»é¡µæ•°"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            # æŸ¥æ‰¾æ‰€æœ‰åˆ†é¡µé“¾æ¥ä¸­çš„æœ€å¤§é¡µç 
            all_links = soup.find_all('a')
            page_numbers = set()
            for link in all_links:
                href = link.get('href', '')
                match = re.search(r'page=(\d+)', href)
                if match:
                    page_numbers.add(int(match.group(1)))
            
            if page_numbers:
                max_page = max(page_numbers)
                return max_page
            return 1
        except Exception as e:
            print(f"âš  æå–é¡µç å¤±è´¥: {e}", file=sys.stderr, flush=True)
            return 1
    
    def extract_tid_from_url(self, url):
        """ä»URLä¸­æå– thread ID"""
        try:
            match = re.search(r'tid=(\d+)', url)
            if match:
                return match.group(1)
            # ä¹Ÿå°è¯•ä» htm_data è·¯å¾„æå–
            match = re.search(r'htm_data/\d+/\d+/(\d+)\.html', url)
            if match:
                return match.group(1)
            return None
        except Exception as e:
            print(f"âš  æå–tidå¤±è´¥: {e}", file=sys.stderr, flush=True)
            return None
    
    def build_pagination_url(self, original_url, page_num):
        """ä¸ºç»™å®šé¡µç æ„å»ºURL"""
        try:
            tid = self.extract_tid_from_url(original_url)
            if tid:
                # ä½¿ç”¨æ ‡å‡†åˆ†é¡µURLæ ¼å¼
                return f"https://t66y.com/read.php?tid={tid}&page={page_num}"
            return None
        except Exception as e:
            print(f"âš  æ„å»ºåˆ†é¡µURLå¤±è´¥: {e}", file=sys.stderr, flush=True)
            return None
    
    def parse_t66y_post(self, url, html, task_type='image'):
        """è§£æ t66y è®ºå›å¸–å­ - æå–æ‰€æœ‰é¡µé¢å’Œæ¥¼å±‚çš„å†…å®¹"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # æå–æ ‡é¢˜ï¼ˆä»…ä»ç¬¬ä¸€é¡µï¼‰
            title = 'æœªçŸ¥æ ‡é¢˜'
            # æŸ¥æ‰¾æ ‡é¢˜ - é€šå¸¸åœ¨ h4.f16 ä¸­
            title_elem = soup.find('h4', class_='f16')
            if not title_elem:
                title_elem = soup.find('h1', class_='bbs-head-title')
            if not title_elem:
                title_elem = soup.find('h1')
            if not title_elem:
                title_elem = soup.find('title')
            
            if title_elem:
                title = title_elem.get_text(strip=True)
                # æ¸…ç†æ ‡é¢˜
                if ' - t66y' in title or ' - ' in title:
                    title = title.split(' - ')[0].strip()
                if title.startswith('Re:'):
                    title = title[3:].strip()
            
            # åˆå¹¶æ‰€æœ‰é¡µé¢å’Œæ¥¼å±‚çš„å†…å®¹å’Œå›¾ç‰‡
            all_content_parts = []
            all_images = []
            
            # ç¬¬ä¸€æ­¥ï¼šæå–ç¬¬ä¸€é¡µå†…å®¹ï¼ˆå·²æœ‰HTMLï¼‰
            print(f"ğŸ“„ å¼€å§‹æå–ç¬¬ä¸€é¡µå†…å®¹...", flush=True)
            all_content_parts, all_images = self._extract_page_content(
                html, all_content_parts, all_images, page_num=1
            )
            
            # ç¬¬äºŒæ­¥ï¼šæ£€æµ‹æ˜¯å¦æœ‰åç»­é¡µé¢
            total_pages = self.extract_page_numbers(html)
            print(f"ğŸ“Š æ£€æµ‹åˆ°æ€»é¡µæ•°: {total_pages}", flush=True)
            
            # ç¬¬ä¸‰æ­¥ï¼šå¦‚æœæœ‰å¤šé¡µï¼Œé€é¡µè·å–å†…å®¹
            if total_pages > 1:
                print(f"ğŸ”„ å¤šåˆ†é¡µæ¨¡å¼ï¼šå¼€å§‹éå†ç¬¬ 2-{total_pages} é¡µ...", flush=True)
                for page_num in range(2, total_pages + 1):
                    # æ„å»ºåˆ†é¡µURL
                    page_url = self.build_pagination_url(url, page_num)
                    if not page_url:
                        print(f"âš  æ— æ³•ä¸ºé¡µé¢ {page_num} æ„å»ºURLï¼Œè·³è¿‡", flush=True)
                        continue
                    
                    # è·å–è¯¥é¡µå†…å®¹
                    page_html = self.fetch_page(page_url)
                    if not page_html:
                        print(f"âš  é¡µé¢ {page_num} è·å–å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€é¡µ", flush=True)
                        continue
                    
                    # æå–å†…å®¹
                    print(f"  â†’ ç¬¬ {page_num} é¡µ: æå–ä¸­...", flush=True)
                    all_content_parts, all_images = self._extract_page_content(
                        page_html, all_content_parts, all_images, page_num=page_num
                    )
            else:
                print(f"ğŸ“„ å•åˆ†é¡µæ¨¡å¼ï¼šä»…æå–ç¬¬ 1 é¡µ", flush=True)
            
            # åˆå¹¶æ‰€æœ‰å†…å®¹ - ç”¨åŒæ¢è¡Œåˆ†éš”ä¸åŒæ¥¼å±‚
            content = '\n\n'.join(all_content_parts) if all_content_parts else 'æš‚æ— å†…å®¹'
            
            return {
                'title': title,
                'content': content,
                'author': 'æ¥¼ä¸»',
                'sourceUrl': url,
                'images': all_images,
            }
        except Exception as e:
            print(f"âœ— è§£æé¡µé¢å¤±è´¥: {e}", file=sys.stderr, flush=True)
            import traceback
            traceback.print_exc()
            return None
    
    def _extract_page_content(self, html, content_parts, images, page_num=1):
        """ä»å•ä¸ªé¡µé¢HTMLä¸­æå–å†…å®¹å’Œå›¾ç‰‡"""
        try:
            soup = BeautifulSoup(html, 'html.parser')
            
            # åœ¨ t66y è®ºå›ä¸­ï¼Œæ¯ä¸ªæ¥¼å±‚éƒ½æ˜¯ä¸€ä¸ª div.tpc_content
            # æ‰¾æ‰€æœ‰çš„ tpc_content divï¼ˆåŒ…æ‹¬ id="conttpc", id="cont..." ç­‰ï¼‰
            content_divs = soup.find_all('div', class_='tpc_content')
            
            if content_divs:
                # å¯¹äºå°è¯´ç±»ä»»åŠ¡ï¼Œæå–æ‰€æœ‰æ¥¼å±‚çš„å†…å®¹
                # å¯¹äºå›¾ç‰‡ç±»ä»»åŠ¡ï¼Œä¹Ÿæå–æ‰€æœ‰æ¥¼å±‚ï¼ˆå¯èƒ½å¤šæ¥¼å‘å›¾ï¼‰
                for floor_idx, content_div in enumerate(content_divs, 1):
                    # æå–æ–‡æœ¬å†…å®¹
                    text_content = content_div.get_text(strip=True)
                    if text_content:
                        content_parts.append(text_content)
                    
                    # æå–å›¾ç‰‡
                    img_elements = content_div.find_all('img')
                    for img_idx, img in enumerate(img_elements, 1):
                        # t66y è®ºå›ä½¿ç”¨ ess-data å±æ€§å­˜å‚¨å®é™…å›¾ç‰‡ URL
                        img_url = img.get('ess-data') or img.get('src') or img.get('data-src')
                        
                        if img_url and img_url.startswith('http'):
                            # è¿‡æ»¤æ‰æ˜ç¡®çš„è¡¨æƒ…ã€å¤´åƒç­‰å°å›¾æ ‡
                            if any(x in img_url.lower() for x in ['emotion', 'icon', 'avatar', 'face']):
                                continue
                            
                            # é¿å…é‡å¤æ·»åŠ åŒä¸€å¼ å›¾ç‰‡
                            if img_url not in [img['url'] for img in images]:
                                images.append({
                                    'url': img_url,
                                    'description': f'ç¬¬{page_num}é¡µ æ¥¼å±‚{floor_idx} å›¾ç‰‡{img_idx}'
                                })
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœæ²¡æ‰¾åˆ°æ ‡å‡†çš„ tpc_content divï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
                content_div = soup.find('div', id='conttpc')
                if content_div:
                    text_content = content_div.get_text(strip=True)
                    if text_content:
                        content_parts.append(text_content)
                    
                    img_elements = content_div.find_all('img')
                    for img in img_elements:
                        img_url = img.get('ess-data') or img.get('src') or img.get('data-src')
                        if img_url and img_url.startswith('http'):
                            if any(x in img_url.lower() for x in ['emotion', 'icon', 'avatar', 'face']):
                                continue
                            if img_url not in [img['url'] for img in images]:
                                images.append({
                                    'url': img_url,
                                    'description': f'å›¾ç‰‡ {len(images) + 1}'
                                })
            
            return content_parts, images
        except Exception as e:
            print(f"âš  æå–é¡µé¢å†…å®¹å¤±è´¥: {e}", file=sys.stderr, flush=True)
            return content_parts, images
        except Exception as e:
            print(f"âœ— è§£æé¡µé¢å¤±è´¥: {e}", file=sys.stderr, flush=True)
            import traceback
            traceback.print_exc()
            return None
    
    def crawl_forum(self, forum_url, task_type='image', max_depth=1):
        """çˆ¬å–è®ºå›å†…å®¹"""
        try:
            print(f"å¼€å§‹çˆ¬è™«ä»»åŠ¡ {self.task_id}", flush=True)
            print(f"URL: {forum_url}", flush=True)
            print(f"Type: {task_type}", flush=True)
            
            # è·å–é¡µé¢
            html = self.fetch_page(forum_url)
            if not html:
                print(f"âœ— æ— æ³•è·å–é¡µé¢å†…å®¹", file=sys.stderr, flush=True)
                return {
                    'success': False,
                    'task_id': self.task_id,
                    'error': 'æ— æ³•è·å–é¡µé¢å†…å®¹',
                }
            
            # è§£æé¡µé¢ï¼ˆè·å–æ‰€æœ‰é¡µé¢çš„æ¥¼ä¸»å†…å®¹ï¼‰
            post_data = self.parse_t66y_post(forum_url, html, task_type)
            if not post_data:
                print(f"âœ— è§£æé¡µé¢å¤±è´¥", file=sys.stderr, flush=True)
                return {
                    'success': False,
                    'task_id': self.task_id,
                    'error': 'è§£æé¡µé¢å¤±è´¥',
                }
            
            # åˆå§‹åŒ–å›¾ç‰‡ç›®å½•
            initialize_image_dirs()
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹å†³å®šæ˜¯å¦ä¿å­˜å†…å®¹
            if task_type == 'novel':
                # æ–‡æœ¬ç±»ï¼šåªä¿å­˜æ–‡æœ¬å†…å®¹ï¼Œä¸ä¿å­˜å›¾ç‰‡
                print(f"âœ“ è·å–æ¥¼ä¸»æ–‡æœ¬å†…å®¹: {len(post_data['content'])} å­—ç¬¦", flush=True)
                media = []
            elif task_type == 'image':
                # å›¾ç‰‡ç±»ï¼šåªä¿å­˜å›¾ç‰‡ï¼Œæ¸…ç©ºæ–‡æœ¬å†…å®¹
                if post_data['images']:
                    print(f"âœ“ è·å–æ¥¼ä¸»å›¾ç‰‡: {len(post_data['images'])} å¼ ", flush=True)
                    
                    # ä¸‹è½½æ‰€æœ‰å›¾ç‰‡
                    print(f"å¼€å§‹ä¸‹è½½å›¾ç‰‡...", flush=True)
                    image_urls = [img['url'] for img in post_data['images']]
                    download_results = download_images(image_urls, self.task_id)
                    
                    # å°†ä¸‹è½½åçš„æœ¬åœ°è·¯å¾„ä¿å­˜åˆ° media
                    media = []
                    success_count = 0
                    for i, result in enumerate(download_results):
                        if result['success']:
                            media.append({
                                'url': result['local_path'],
                                'originalUrl': image_urls[i],
                                'description': f'æ¥¼ä¸»å›¾ç‰‡ {i + 1}'
                            })
                            success_count += 1
                        else:
                            print(f"âš  å›¾ç‰‡ä¸‹è½½å¤±è´¥ {i + 1}: {result['error']}", flush=True)
                    
                    print(f"âœ“ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {success_count}/{len(post_data['images'])} æˆåŠŸ", flush=True)
                else:
                    print(f"âš  æ¥¼ä¸»æœªå‘å¸ƒå›¾ç‰‡ï¼Œä½¿ç”¨å ä½ç¬¦", flush=True)
                    media = [{
                        'url': 'https://via.placeholder.com/300x200?text=No+Image',
                        'description': 'æ¥¼ä¸»æœªå‘å¸ƒå›¾ç‰‡'
                    }]
                # å›¾ç‰‡ç±»ä¸ä¿å­˜æ–‡æœ¬ï¼Œåªä¿å­˜æ ‡é¢˜
                post_data['content'] = f"æ¥¼ä¸»å‘å¸ƒäº† {len(post_data['images'])} å¼ å›¾ç‰‡"
            else:  # mixed
                # æ··åˆç±»ï¼šæ—¢ä¿å­˜æ–‡æœ¬ä¹Ÿä¿å­˜å›¾ç‰‡
                print(f"âœ“ è·å–æ¥¼ä¸»å†…å®¹: {len(post_data['content'])} å­—ç¬¦, {len(post_data['images'])} å¼ å›¾ç‰‡", flush=True)
                
                if post_data['images']:
                    # ä¸‹è½½æ‰€æœ‰å›¾ç‰‡
                    print(f"å¼€å§‹ä¸‹è½½å›¾ç‰‡...", flush=True)
                    image_urls = [img['url'] for img in post_data['images']]
                    download_results = download_images(image_urls, self.task_id)
                    
                    # å°†ä¸‹è½½åçš„æœ¬åœ°è·¯å¾„ä¿å­˜åˆ° media
                    media = []
                    success_count = 0
                    for i, result in enumerate(download_results):
                        if result['success']:
                            media.append({
                                'url': result['local_path'],
                                'originalUrl': image_urls[i],
                                'description': f'æ¥¼ä¸»å›¾ç‰‡ {i + 1}'
                            })
                            success_count += 1
                        else:
                            print(f"âš  å›¾ç‰‡ä¸‹è½½å¤±è´¥ {i + 1}: {result['error']}", flush=True)
                    
                    print(f"âœ“ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {success_count}/{len(post_data['images'])} æˆåŠŸ", flush=True)
                else:
                    media = []
            
            # æ„å»º MongoDB æ–‡æ¡£
            post = {
                'title': post_data['title'],
                'content': post_data['content'],
                'author': post_data['author'],
                'sourceUrl': forum_url,
                'postType': 'image' if task_type == 'image' else 'novel' if task_type == 'novel' else 'text',
                'likes': 0,
                'views': 0,
                'replies': 0,
                'status': 'active',
                'tags': [task_type, 't66y'],
                'taskId': ObjectId(self.task_id),
                'createdAt': datetime.now(timezone.utc),
            }
            
            # æ·»åŠ åª’ä½“ä¿¡æ¯
            if media:
                post['media'] = media
            else:
                # å¦‚æœæ²¡æœ‰åª’ä½“ï¼Œæ·»åŠ å ä½ç¬¦
                post['media'] = [{
                    'url': 'https://via.placeholder.com/300x200?text=No+Content',
                    'description': 'æš‚æ— åª’ä½“å†…å®¹'
                }]
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            try:
                # ä½¿ç”¨ upsert æ–¹å¼ï¼Œé¿å…é‡å¤é”®é”™è¯¯
                result = self.posts_collection.update_one(
                    {'sourceUrl': forum_url},  # æŸ¥è¯¢æ¡ä»¶
                    {
                        '$set': {
                            'title': post['title'],
                            'content': post['content'],
                            'author': post['author'],
                            'postType': post['postType'],
                            'likes': post['likes'],
                            'views': post['views'],
                            'replies': post['replies'],
                            'status': post['status'],
                            'tags': post['tags'],
                            'taskId': post['taskId'],
                            'media': post['media'],
                            'updatedAt': datetime.now(timezone.utc),
                        },
                        '$setOnInsert': {
                            'createdAt': datetime.now(timezone.utc),
                        }
                    },
                    upsert=True  # å¦‚æœä¸å­˜åœ¨åˆ™æ’å…¥
                )
                print(f"âœ“ æ–‡ç« å·²ä¿å­˜: {post['title']}", flush=True)
                print(f"TITLE:{post['title']}", flush=True)
                print(f"PROGRESS:100", flush=True)
                print(f"CRAWLED:1", flush=True)
                
                return {
                    'success': True,
                    'task_id': self.task_id,
                    'total_posts': 1,
                    'message': 'çˆ¬è™«ä»»åŠ¡å®Œæˆ'
                }
            except Exception as e:
                print(f"âœ— ä¿å­˜æ•°æ®åº“å¤±è´¥: {e}", file=sys.stderr, flush=True)
                import traceback
                traceback.print_exc()
                return {
                    'success': False,
                    'task_id': self.task_id,
                    'error': str(e),
                }
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹å†³å®šæ˜¯å¦ä¿å­˜å†…å®¹
            if task_type == 'novel':
                # æ–‡æœ¬ç±»ï¼šåªä¿å­˜æ–‡æœ¬å†…å®¹ï¼Œä¸ä¿å­˜å›¾ç‰‡
                print(f"âœ“ è·å–æ¥¼ä¸»æ–‡æœ¬å†…å®¹: {len(post_data['content'])} å­—ç¬¦", flush=True)
                media = []
            elif task_type == 'image':
                # å›¾ç‰‡ç±»ï¼šåªä¿å­˜å›¾ç‰‡ï¼Œæ¸…ç©ºæ–‡æœ¬å†…å®¹
                if post_data['images']:
                    print(f"âœ“ è·å–æ¥¼ä¸»å›¾ç‰‡: {len(post_data['images'])} å¼ ", flush=True)
                    
                    # ä¸‹è½½æ‰€æœ‰å›¾ç‰‡
                    print(f"å¼€å§‹ä¸‹è½½å›¾ç‰‡...", flush=True)
                    image_urls = [img['url'] for img in post_data['images']]
                    download_results = download_images(image_urls, self.task_id)
                    
                    # å°†ä¸‹è½½åçš„æœ¬åœ°è·¯å¾„ä¿å­˜åˆ° media
                    media = []
                    success_count = 0
                    for i, result in enumerate(download_results):
                        if result['success']:
                            media.append({
                                'url': result['local_path'],
                                'originalUrl': image_urls[i],
                                'description': f'æ¥¼ä¸»å›¾ç‰‡ {i + 1}'
                            })
                            success_count += 1
                        else:
                            print(f"âš  å›¾ç‰‡ä¸‹è½½å¤±è´¥ {i + 1}: {result['error']}", flush=True)
                    
                    print(f"âœ“ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {success_count}/{len(post_data['images'])} æˆåŠŸ", flush=True)
                else:
                    print(f"âš  æ¥¼ä¸»æœªå‘å¸ƒå›¾ç‰‡ï¼Œä½¿ç”¨å ä½ç¬¦", flush=True)
                    media = [{
                        'url': 'https://via.placeholder.com/300x200?text=No+Image',
                        'description': 'æ¥¼ä¸»æœªå‘å¸ƒå›¾ç‰‡'
                    }]
                # å›¾ç‰‡ç±»ä¸ä¿å­˜æ–‡æœ¬ï¼Œåªä¿å­˜æ ‡é¢˜
                post_data['content'] = f"æ¥¼ä¸»å‘å¸ƒäº† {len(post_data['images'])} å¼ å›¾ç‰‡"
            else:  # mixed
                # æ··åˆç±»ï¼šæ—¢ä¿å­˜æ–‡æœ¬ä¹Ÿä¿å­˜å›¾ç‰‡
                print(f"âœ“ è·å–æ¥¼ä¸»å†…å®¹: {len(post_data['content'])} å­—ç¬¦, {len(post_data['images'])} å¼ å›¾ç‰‡", flush=True)
                
                if post_data['images']:
                    # ä¸‹è½½æ‰€æœ‰å›¾ç‰‡
                    print(f"å¼€å§‹ä¸‹è½½å›¾ç‰‡...", flush=True)
                    image_urls = [img['url'] for img in post_data['images']]
                    download_results = download_images(image_urls, self.task_id)
                    
                    # å°†ä¸‹è½½åçš„æœ¬åœ°è·¯å¾„ä¿å­˜åˆ° media
                    media = []
                    success_count = 0
                    for i, result in enumerate(download_results):
                        if result['success']:
                            media.append({
                                'url': result['local_path'],
                                'originalUrl': image_urls[i],
                                'description': f'æ¥¼ä¸»å›¾ç‰‡ {i + 1}'
                            })
                            success_count += 1
                        else:
                            print(f"âš  å›¾ç‰‡ä¸‹è½½å¤±è´¥ {i + 1}: {result['error']}", flush=True)
                    
                    print(f"âœ“ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {success_count}/{len(post_data['images'])} æˆåŠŸ", flush=True)
                else:
                    media = []
            
            # æ„å»º MongoDB æ–‡æ¡£
            post = {
                'title': post_data['title'],
                'content': post_data['content'],
                'author': post_data['author'],
                'sourceUrl': forum_url,
                'postType': 'image' if task_type == 'image' else 'novel' if task_type == 'novel' else 'text',
                'likes': 0,
                'views': 0,
                'replies': 0,
                'status': 'active',
                'tags': [task_type, 't66y'],
                'taskId': ObjectId(self.task_id),
                'createdAt': datetime.now(timezone.utc),
            }
            
            # æ·»åŠ åª’ä½“ä¿¡æ¯
            if media:
                post['media'] = media
            else:
                # å¦‚æœæ²¡æœ‰åª’ä½“ï¼Œæ·»åŠ å ä½ç¬¦
                post['media'] = [{
                    'url': 'https://via.placeholder.com/300x200?text=No+Content',
                    'description': 'æš‚æ— åª’ä½“å†…å®¹'
                }]
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            try:
                # ä½¿ç”¨ upsert æ–¹å¼ï¼Œé¿å…é‡å¤é”®é”™è¯¯
                result = self.posts_collection.update_one(
                    {'sourceUrl': forum_url},  # æŸ¥è¯¢æ¡ä»¶
                    {
                        '$set': {
                            'title': post['title'],
                            'content': post['content'],
                            'author': post['author'],
                            'postType': post['postType'],
                            'likes': post['likes'],
                            'views': post['views'],
                            'replies': post['replies'],
                            'status': post['status'],
                            'tags': post['tags'],
                            'taskId': post['taskId'],
                            'media': post['media'],
                            'updatedAt': datetime.now(timezone.utc),
                        },
                        '$setOnInsert': {
                            'createdAt': datetime.now(timezone.utc),
                        }
                    },
                    upsert=True  # å¦‚æœä¸å­˜åœ¨åˆ™æ’å…¥
                )
                print(f"âœ“ æ–‡ç« å·²ä¿å­˜: {post['title']}", flush=True)
                print(f"TITLE:{post['title']}", flush=True)
                print(f"PROGRESS:100", flush=True)
                print(f"CRAWLED:1", flush=True)
                
                return {
                    'success': True,
                    'task_id': self.task_id,
                    'total_posts': 1,
                    'message': 'çˆ¬è™«ä»»åŠ¡å®Œæˆ'
                }
            except Exception as e:
                print(f"âœ— ä¿å­˜æ•°æ®åº“å¤±è´¥: {e}", file=sys.stderr, flush=True)
                import traceback
                traceback.print_exc()
                return {
                    'success': False,
                    'task_id': self.task_id,
                    'error': str(e),
                }
        
        except Exception as e:
            print(f"âœ— çˆ¬è™«æ‰§è¡Œå¤±è´¥: {e}", file=sys.stderr, flush=True)
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'task_id': self.task_id,
                'error': str(e),
            }
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.client:
            self.client.close()

def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(description='Forum Crawler')
    parser.add_argument('--url', required=True, help='è®ºå› URL')
    parser.add_argument('--type', default='mixed', help='çˆ¬è™«ç±»å‹ (novel, image, mixed)')
    parser.add_argument('--task-id', required=True, help='ä»»åŠ¡ ID')
    parser.add_argument('--max-depth', type=int, default=1, help='æœ€å¤§æ·±åº¦')
    parser.add_argument('--delay', type=int, default=1000, help='è¯·æ±‚å»¶è¿Ÿ')
    parser.add_argument('--timeout', type=int, default=600000, help='è¶…æ—¶æ—¶é—´ (ms)')
    
    args = parser.parse_args()
    
    # è·å– MongoDB URI
    mongodb_uri = os.environ.get(
        'MONGODB_URI',
        'mongodb://admin:admin123@mongo:27017/forum-crawler?authSource=admin'
    )
    
    crawler = None
    try:
        crawler = ForumCrawler(args.task_id, mongodb_uri)
        result = crawler.crawl_forum(args.url, args.type, args.max_depth)
        
        if result['success']:
            print(f"CRAWLED:{result.get('total_posts', 0)}", flush=True)
            sys.exit(0)
        else:
            print(f"ERROR:{result.get('error')}", file=sys.stderr, flush=True)
            sys.exit(1)
    
    except KeyboardInterrupt:
        print("çˆ¬è™«è¢«ä¸­æ–­", file=sys.stderr, flush=True)
        sys.exit(130)
    except Exception as e:
        print(f"ERROR:{str(e)}", file=sys.stderr, flush=True)
        sys.exit(1)
    finally:
        if crawler:
            crawler.close()

if __name__ == '__main__':
    main()
